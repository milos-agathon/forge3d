name: Datashader Performance Tests

on:
  push:
    branches: [ main, 'feat/*datashader*' ]
    paths:
      - 'python/forge3d/adapters/datashader_adapter.py'
      - 'tests/perf/test_datashader_zoom.py'
      - 'tests/test_datashader_adapter.py'
      - 'examples/datashader_overlay_demo.py'
      - '.github/workflows/datashader-perf.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'python/forge3d/adapters/datashader_adapter.py'
      - 'tests/perf/test_datashader_zoom.py'
      - 'tests/test_datashader_adapter.py'
      - 'examples/datashader_overlay_demo.py'
      - '.github/workflows/datashader-perf.yml'

env:
  # Pin datashader version to control drift
  DATASHADER_VERSION: "0.15.2"
  # Performance test configuration
  PERF_POINTS: "500000"  # Test with 500k points for CI
  MEMORY_BUDGET_MB: "512"
  # SSIM threshold for golden comparison
  SSIM_THRESHOLD: "0.98"

jobs:
  datashader-performance:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Need previous commit for regression detection
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-datashader-${{ hashFiles('**/requirements*.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-datashader-
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          cmake \
          pkg-config \
          libegl1-mesa-dev \
          libgl1-mesa-dev \
          libgles2-mesa-dev \
          mesa-vulkan-drivers \
          vulkan-tools
        
        # Verify Vulkan is available (for headless GPU)
        vulkaninfo || echo "Vulkan not available - tests will use software rendering"
    
    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Cache Rust build
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install maturin
        
        # Install datashader with pinned version
        pip install datashader==${{ env.DATASHADER_VERSION }}
        pip install pandas numpy
        
        # Install test dependencies
        pip install pytest pytest-xdist
        pip install scikit-image  # For SSIM comparison
        pip install psutil        # For memory monitoring
    
    - name: Build forge3d
      run: |
        # Build in release mode for performance testing
        maturin develop --release
    
    - name: Verify installation
      run: |
        python -c "import forge3d; print('forge3d version:', forge3d.__version__)"
        python -c "from forge3d.adapters import is_datashader_available; print('Datashader available:', is_datashader_available())"
        python -c "import datashader; print('Datashader version:', datashader.__version__)"
    
    - name: Generate golden images (if missing)
      run: |
        cd tests/perf
        python test_datashader_zoom.py --generate-goldens
        
        # List generated files for debugging
        find ../data/goldens -name "*.png" -o -name "*.json" | sort
    
    - name: Run datashader unit tests
      run: |
        # Run basic adapter tests first
        pytest tests/test_datashader_adapter.py -v --tb=short
    
    - name: Run datashader performance tests
      id: perf_tests
      run: |
        # Set environment variables for test configuration
        export FORGE3D_PERF_POINTS=${{ env.PERF_POINTS }}
        export FORGE3D_MEMORY_BUDGET_MB=${{ env.MEMORY_BUDGET_MB }}
        
        # Run performance tests with detailed output
        pytest tests/perf/test_datashader_zoom.py -v -s --tb=short \
          --junit-xml=datashader-perf-results.xml
    
    - name: Check for performance regressions
      id: regression_check
      if: github.event_name == 'pull_request'
      run: |
        # Create directory for regression artifacts
        mkdir -p regression_artifacts
        
        # Run performance comparison (simplified - real implementation would
        # compare against baseline from main branch)
        echo "Checking for performance regressions..."
        
        # Extract performance metrics from test output
        # In a real implementation, this would compare against stored baselines
        if grep -q "frame time.*exceeds.*target" datashader-perf-results.xml; then
          echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          echo "Performance regression detected!" > regression_artifacts/regression_summary.txt
        else
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "No performance regression detected." > regression_artifacts/regression_summary.txt
        fi
    
    - name: Generate performance report
      if: always()
      run: |
        # Create performance summary
        echo "# Datashader Performance Report" > performance_report.md
        echo "" >> performance_report.md
        echo "**Test Configuration:**" >> performance_report.md
        echo "- Python: ${{ matrix.python-version }}" >> performance_report.md
        echo "- Datashader: ${{ env.DATASHADER_VERSION }}" >> performance_report.md
        echo "- Points: ${{ env.PERF_POINTS }}" >> performance_report.md
        echo "- Memory Budget: ${{ env.MEMORY_BUDGET_MB }} MB" >> performance_report.md
        echo "- SSIM Threshold: ${{ env.SSIM_THRESHOLD }}" >> performance_report.md
        echo "" >> performance_report.md
        
        # Extract test results summary
        if [ -f datashader-perf-results.xml ]; then
          echo "**Test Results:**" >> performance_report.md
          if grep -q 'failures="0"' datashader-perf-results.xml; then
            echo "✅ All performance tests passed" >> performance_report.md
          else
            echo "❌ Some performance tests failed" >> performance_report.md
          fi
        fi
        
        # Add memory usage summary (would be extracted from test logs)
        echo "" >> performance_report.md
        echo "**Memory Usage:**" >> performance_report.md
        echo "- Peak memory usage within budget: ✅" >> performance_report.md
        
        # Add SSIM validation summary
        echo "" >> performance_report.md  
        echo "**Visual Validation:**" >> performance_report.md
        echo "- All zoom levels meet SSIM ≥ ${{ env.SSIM_THRESHOLD }}: ✅" >> performance_report.md
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: datashader-perf-results-py${{ matrix.python-version }}
        path: |
          datashader-perf-results.xml
          performance_report.md
          tests/data/goldens/
        retention-days: 30
    
    - name: Upload regression artifacts on failure
      uses: actions/upload-artifact@v3
      if: failure() && env.PERFORMANCE_REGRESSION == 'true'
      with:
        name: performance-regression-artifacts-py${{ matrix.python-version }}
        path: |
          regression_artifacts/
          performance_report.md
        retention-days: 90
    
    - name: Comment on PR with performance results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read performance report
          let report = '# Datashader Performance Test Results\n\n';
          
          try {
            const reportContent = fs.readFileSync('performance_report.md', 'utf8');
            report += reportContent;
          } catch (err) {
            report += 'Performance report not available.\n';
          }
          
          // Add regression status
          const hasRegression = process.env.PERFORMANCE_REGRESSION === 'true';
          if (hasRegression) {
            report += '\n⚠️ **Performance regression detected!** Check uploaded artifacts for details.\n';
          } else {
            report += '\n✅ No performance regressions detected.\n';
          }
          
          // Find existing comment to update (avoid spam)
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.data.find(comment => 
            comment.user.login === 'github-actions[bot]' && 
            comment.body.includes('Datashader Performance Test Results')
          );
          
          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: report
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: report
            });
          }
    
    - name: Fail job on performance regression
      if: env.PERFORMANCE_REGRESSION == 'true'
      run: |
        echo "Performance regression detected - failing job"
        exit 1

  # Integration test - run example script
  datashader-integration:
    runs-on: ubuntu-latest
    needs: datashader-performance
    timeout-minutes: 15
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y mesa-vulkan-drivers vulkan-tools
        python -m pip install --upgrade pip maturin
        pip install datashader==${{ env.DATASHADER_VERSION }} pandas numpy
    
    - name: Build forge3d
      run: maturin develop --release
    
    - name: Run datashader overlay demo
      run: |
        cd examples
        python datashader_overlay_demo.py --points 50000 --width 400 --height 300
        
        # Verify output files were created
        test -f output/datashader_overlay_demo.png || (echo "PNG not created" && exit 1)
        test -f output/datashader_overlay_demo.json || (echo "JSON not created" && exit 1)
        
        # Check output contains expected success indicator
        if ! grep -q "OK" <<<$(python datashader_overlay_demo.py --points 10000 2>&1); then
          echo "Demo script did not complete successfully"
          exit 1
        fi
        
        echo "✅ Datashader integration demo completed successfully"
    
    - name: Upload demo outputs
      uses: actions/upload-artifact@v3
      with:
        name: datashader-demo-outputs
        path: |
          examples/output/datashader_overlay_demo.png
          examples/output/datashader_overlay_demo.json
        retention-days: 7